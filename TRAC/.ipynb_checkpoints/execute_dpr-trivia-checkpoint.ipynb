{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6,7\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import opensource\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "# from trl import SFTTrainer\n",
    "import tasks\n",
    "import utils\n",
    "# from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='trivia'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup opensource model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463e07ddaf154419ab1aacb8abfe30d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\"\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# Fine-tuned model name\n",
    "new_model = f\"llama-2-7b-shuo-{task}-new\"\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_new = PeftModel.from_pretrained(base_model, new_model)\n",
    "# model_new = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "pipe_new = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model_new, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=300,\n",
    "    return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "#         print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def save_results(task):\n",
    "    # save retrieved_scores to a pickle file\n",
    "    write_list(retrieved_scores, f'retrieved_scores_{task}.p')\n",
    "    # save retrieved_true_scores to a pickle file\n",
    "    write_list(retrieved_true_scores, f'retrieved_true_scores_{task}.p')\n",
    "    # save queries to a pickle file\n",
    "    write_list(queries, f'queries_{task}.p')\n",
    "    # save answers to a pickle file\n",
    "    write_list(answers, f'answers_{task}.p')\n",
    "    # save passages to a pickle file\n",
    "    write_list(passages, f'passages_{task}.p')\n",
    "    # save opensource_true_scores to a pickle file\n",
    "    write_list(opensource_true_scores, f'opensource_true_scores_{task}.p')\n",
    "    # save opensource_texts to a pickle file\n",
    "#     write_list(opensource_texts, f'opensource_texts_{task}.p')\n",
    "    # save opensource_answers to a pickle file\n",
    "    write_list(opensource_answers, f'opensource_answers_{task}.p')\n",
    "    # save opensource_semantics to a picle file\n",
    "    write_list(opensource_semantics, f'opensource_semantics_{task}.p')\n",
    "    # save feasibilities to a pickle file\n",
    "    write_list(feasibilities, f'feasibilities_{task}.p')\n",
    "    # save occurances to a pickle file\n",
    "    write_list(occurances, f'occurances_{task}.p')\n",
    "    # save semantic_ids to a pickle file\n",
    "    write_list(semantic_ids, f'semantic_ids_{task}.p')\n",
    "    # save probs to a picle file\n",
    "    write_list(probs, f'probs_{task}.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lishuo1/anaconda3/envs/TRAC/lib/python3.9/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "queries = []\n",
    "answers = []\n",
    "passages = []\n",
    "retrieved_scores = []\n",
    "retrieved_true_scores = []\n",
    "opensource_true_scores = []\n",
    "opensource_texts = []\n",
    "opensource_answers = []\n",
    "opensource_semantics = []\n",
    "semantic_probs = []\n",
    "feasibilities = []\n",
    "occurances = []\n",
    "semantic_ids = []\n",
    "probs = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(elements, scores, retrieved_examples)):\n",
    "        print(f'{idx}, {task}', file=open(f'{task}.txt', 'a'))\n",
    "        feasible = False\n",
    "        if idx % 10 == 0:\n",
    "            save_results(task)\n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element, dataset=task)\n",
    "        if len(passage_id) == 0:\n",
    "            continue\n",
    "        retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "            utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "        if len(true_score) == 0:\n",
    "            continue\n",
    "        \n",
    "        prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "        try:\n",
    "            sequences = opensource.ask_openmodel(prompt, pipe_new, tokenizer, return_sequences=30)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            tmp = seq['generated_text']\n",
    "            idx = np.char.find(tmp, \"~!~\", start=0, end=None)\n",
    "            tmp = tmp[:idx].strip()\n",
    "            generated_texts.append(tmp)\n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        if len(true_scores) == 0:\n",
    "            continue\n",
    "\n",
    "        # collect all answers\n",
    "        prompts = []\n",
    "        for ctx in retrieved_texts:\n",
    "            prompt = utils.get_prompt_template(query, ctx, task=\"Natural Questions\")\n",
    "            prompts.append(prompt)\n",
    "        try:\n",
    "            sequences_list = opensource.ask_openmodel(prompts, pipe_new, tokenizer, return_sequences=30)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "            \n",
    "        feasible = True\n",
    "        retrieved_scores.append(score)\n",
    "        retrieved_true_scores.append(true_score)\n",
    "        queries.append(query)\n",
    "        answers.append(answer)\n",
    "        passages.append(passage_text)\n",
    "        opensource_true_scores.append(true_scores)\n",
    "        \n",
    "        \n",
    "        probs_tmp = []\n",
    "        answers_tmp = []\n",
    "        semantic_id_tmp = []\n",
    "        occurance_tmp = []\n",
    "        semantic_tmp = []\n",
    "        for ctx_idx, (context, s, sequences) in enumerate(zip(retrieved_texts, score, sequences_list)):\n",
    "            generated_texts = []\n",
    "            for seq in sequences:\n",
    "                tmp = seq['generated_text']\n",
    "                idx = np.char.find(tmp, \"~!~\", start=0, end=None)\n",
    "                tmp = tmp[:idx].strip()\n",
    "                generated_texts.append(tmp)\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "            probs_tmp.append(semantic_probs)\n",
    "            answers_tmp.append(generated_texts)\n",
    "            occurance_tmp.append(item_occurance)\n",
    "            semantic_id_tmp.append(semantic_set_ids)\n",
    "        opensource_answers.append(answers_tmp)\n",
    "        feasibilities.append(feasible)\n",
    "        occurances.append(occurance_tmp)\n",
    "        semantic_ids.append(semantic_id_tmp)\n",
    "        probs.append(probs_tmp)\n",
    "print('Finished!', file=open(f'chatgpt_{task}.txt', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
